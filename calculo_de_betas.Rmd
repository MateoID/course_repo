---
title: "R Notebook"
output: null
---


```{r}
require("pacman")

p_load("tidyverse","stargazer")
```
```{r}
dta<-tibble(lnwage=c(5,10,12.50),educ=c(8,12,16))
```
```{r}
ggplot(dta,aes(x=educ,y=lnwage)) +
  geom_point(alpha=1,size=4) +
  theme_bw()  +
  xlab("Education") +
  ylab("Ln(Wage)") +
  xlim(0,20) +
  ylim(-5,20) +
  theme(legend.position = "none",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        text = element_text(size=20)
  )
```
```{r}
# dependant variable as a one-column matrix, i.e., a vector
y<-matrix(dta$lnwage,ncol=1)

# the design matrix
X<-model.matrix(~educ,data=dta)
```

$$
\hat{\beta} = (X'X)^{-1} X'y
$$
Método 1: cálculo de coeficientes a través de la inversa de X'X, la cual será de dimensión k*k. Con grándes volúmenes de datos la inversión de matrices es bastante costosa.
```{r}
beta_coef<-solve(t(X)%*%X)%*%t(X)%*%y

beta_coef
```
Método 2: lm() es el método de cálculo de coeficientes principal de R. Resulta costoso computacionalmente, ya que almacena un dataframe con los mismos datos que la tabla incluida para el argumento "data=".
```{r}
reg_with_lm<- lm(lnwage~educ,data=dta)
reg_with_lm
```
lm.fit() es la función específica que R usa para realizar la regresión lineal.
```{r}
lm.fit(X,y)$coefficients
```
lm() parte de la expresieste punto para descomponer X'X en las matrices Q y R asi:
$$
X'X\hat{\beta} = X'y \\
(R'Q'QR)\hat{\beta} = R'Q'y \\
(R'R)\hat{\beta} = R'Q'y \\
R\hat{\beta} = Q'y
$$

```{r}
QR<-qr(X)
R<- qr.R(QR)
R
```
```{r}
Q<- qr.Q(QR)
Q
```
```{r}
t(Q)%*%y
```
```{r}
beta1 <- -5.303301 / -5.656854
beta1
```
```{r}
beta0 <- (-15.877132 - (-20.784610*beta1)) / -1.732051
beta0
```
